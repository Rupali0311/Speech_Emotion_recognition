{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.utils import shuffle\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import glob "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rav_sav= os.listdir('Dataset1/')\n",
    "tess= os.listdir('Dataset2/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing size of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920\n",
      "2800\n"
     ]
    }
   ],
   "source": [
    "print(len(rav_sav))\n",
    "print(len(tess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping the data of RAVDESS and SAVEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions=[]\n",
    "for file in rav_sav:\n",
    "    if file[6:-16]=='01':\n",
    "        emotions.append('neutral')\n",
    "    elif file[6:-16]=='03' and int(file[18:-4])%2==0:\n",
    "        emotions.append('happy')\n",
    "    elif file[6:-16]=='03' and int(file[18:-4])%2==1:\n",
    "        emotions.append('happy')\n",
    "    elif file[6:-16]=='04' and int(file[18:-4])%2==0:\n",
    "        emotions.append('sad')\n",
    "    elif file[6:-16]=='04' and int(file[18:-4])%2==1:\n",
    "        emotions.append('sad')\n",
    "    elif file[6:-16]=='05' and int(file[18:-4])%2==0:\n",
    "        emotions.append('angry')\n",
    "    elif file[6:-16]=='05' and int(file[18:-4])%2==1:\n",
    "        emotions.append('angry')\n",
    "    elif file[6:-16]=='06' and int(file[18:-4])%2==0:\n",
    "        emotions.append('fearful')\n",
    "    elif file[6:-16]=='06' and int(file[18:-4])%2==1:\n",
    "        emotions.append('fearful')\n",
    "    elif file[6:-16]=='07':\n",
    "        emotions.append('disgust')\n",
    "    elif file[:1]=='a':\n",
    "        emotions.append('angry')\n",
    "    elif file[:1]=='f':\n",
    "        emotions.append('fearful')\n",
    "    elif file[:1]=='h':\n",
    "        emotions.append('happy')\n",
    "    elif file[:2]=='sa':\n",
    "        emotions.append('sad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping the data of TESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tess:\n",
    "    if file[-11:-4]=='neutral':\n",
    "        emotions.append('neutral')\n",
    "    elif file[-9:-4]=='angry':\n",
    "        emotions.append('angry')\n",
    "    elif file[-11:-4]=='disgust':\n",
    "        emotions.append('disgust')\n",
    "    elif file[-9:-4]=='happy':\n",
    "        emotions.append('happy')\n",
    "    elif file[-7:-4]=='sad':\n",
    "        emotions.append('sad')\n",
    "    elif file[-8:-4]=='fear':\n",
    "        emotions.append('fearful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction of RAVDESS and SAVEE dataset using mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['feature'])\n",
    "count=0\n",
    "for i,y in enumerate(rav_sav):\n",
    "    if rav_sav[i][6:-16]!='02' and rav_sav[i][6:-16]!='08' and rav_sav[i][:2]!='su' and rav_sav[i][:1]!='n' and rav_sav[i][:1]!='d':\n",
    "        X, sample_rate = librosa.load('Dataset1/'+y, res_type='kaiser_fast',duration=2.5,sr=22050,offset=0)\n",
    "        sample_rate = np.array(sample_rate)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\n",
    "        feature = mfccs\n",
    "        df.loc[count] = [feature]\n",
    "        count=count+1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction of TESS dataset using mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,y in enumerate(tess):\n",
    "     if tess[i][-6:-4]!='ps':\n",
    "        X, sample_rate = librosa.load('Dataset2/'+y, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0)\n",
    "        sample_rate = np.array(sample_rate)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X,  sr=sample_rate,  n_mfcc=13), axis=0)\n",
    "        feature = mfccs\n",
    "        df.loc[count] = [feature]\n",
    "        count=count+1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                feature\n",
      "0     [-65.946724, -65.946724, -65.946724, -65.94672...\n",
      "1     [-63.72445, -63.72445, -63.72445, -63.72445, -...\n",
      "2     [-59.59301, -59.59301, -59.59301, -59.59301, -...\n",
      "3     [-66.73365, -66.73365, -66.73365, -66.73365, -...\n",
      "4     [-61.566444, -57.94428, -56.239346, -58.175125...\n",
      "...                                                 ...\n",
      "3691  [-43.600605, -43.108433, -42.603493, -43.46729...\n",
      "3692  [-31.2706, -23.65622, -21.566856, -22.070164, ...\n",
      "3693  [-32.686943, -30.507328, -27.698042, -31.79878...\n",
      "3694  [-41.914738, -40.3994, -39.37127, -39.626366, ...\n",
      "3695  [-33.994755, -35.7186, -36.789467, -34.117706,...\n",
      "\n",
      "[3696 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdf = pd.DataFrame(df['feature'].values.tolist())\n",
    "df_new = pd.concat([tempdf,labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ren_df = df_new.rename(index=str, columns={\"0\": \"label\"})\n",
    "ren_df = shuffle(df_new)\n",
    "ren_df = ren_df.fillna(0)\n",
    "final_df = np.random.rand(len(ren_df)) < 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2948\n",
      "748\n"
     ]
    }
   ],
   "source": [
    "train = ren_df[final_df]\n",
    "test = ren_df[~final_df]\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0          1          2          3          4          5    \\\n",
      "0    -65.946724 -65.946724 -65.946724 -65.946724 -65.946724 -65.946724   \n",
      "1    -63.724449 -63.724449 -63.724449 -63.724449 -63.724449 -63.724449   \n",
      "2    -59.593010 -59.593010 -59.593010 -59.593010 -59.593010 -59.593010   \n",
      "3    -66.733650 -66.733650 -66.733650 -66.733650 -66.733650 -66.733650   \n",
      "4    -61.566444 -57.944279 -56.239346 -58.175125 -58.332890 -60.323990   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "3691 -43.600605 -43.108433 -42.603493 -43.467293 -41.499535 -41.084091   \n",
      "3692 -31.270599 -23.656219 -21.566856 -22.070164 -24.841099 -28.609322   \n",
      "3693 -32.686943 -30.507328 -27.698042 -31.798786 -33.790195 -32.447849   \n",
      "3694 -41.914738 -40.399399 -39.371269 -39.626366 -35.696751 -28.868685   \n",
      "3695 -33.994755 -35.718601 -36.789467 -34.117706 -30.381868 -28.359896   \n",
      "\n",
      "            6          7          8          9    ...        206        207  \\\n",
      "0    -65.946724 -65.752853 -65.946724 -65.946724  ...        NaN        NaN   \n",
      "1    -63.724449 -63.724449 -63.724449 -63.724449  ...        NaN        NaN   \n",
      "2    -59.593010 -59.593010 -59.593010 -59.593010  ...        NaN        NaN   \n",
      "3    -66.733650 -66.733650 -66.733650 -66.733650  ...        NaN        NaN   \n",
      "4    -61.112751 -58.362244 -58.234436 -59.695816  ...        NaN        NaN   \n",
      "...         ...        ...        ...        ...  ...        ...        ...   \n",
      "3691 -38.763359 -37.168015 -38.470959 -41.190334  ... -30.920593 -32.094967   \n",
      "3692 -31.833010 -33.966671 -35.018528 -34.283588  ...        NaN        NaN   \n",
      "3693 -32.784134 -32.587418 -32.316013 -29.904999  ...        NaN        NaN   \n",
      "3694 -27.069733 -25.934561 -26.704355 -28.264874  ...        NaN        NaN   \n",
      "3695 -28.562948 -28.232851 -28.796274 -32.348175  ...        NaN        NaN   \n",
      "\n",
      "            208        209        210        211        212        213  \\\n",
      "0           NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "1           NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "2           NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "3           NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "4           NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "3691 -34.578396 -36.762932 -38.023891 -35.574318 -34.405258 -38.151634   \n",
      "3692        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "3693        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "3694        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "3695        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "\n",
      "            214        215  \n",
      "0           NaN        NaN  \n",
      "1           NaN        NaN  \n",
      "2           NaN        NaN  \n",
      "3           NaN        NaN  \n",
      "4           NaN        NaN  \n",
      "...         ...        ...  \n",
      "3691 -38.480564 -37.453854  \n",
      "3692        NaN        NaN  \n",
      "3693        NaN        NaN  \n",
      "3694        NaN        NaN  \n",
      "3695        NaN        NaN  \n",
      "\n",
      "[3696 rows x 216 columns]\n"
     ]
    }
   ],
   "source": [
    "print(tempdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train.iloc[:, :-1]\n",
    "train_label = train.iloc[:, -1:]\n",
    "test_set = test.iloc[:, :-1]\n",
    "test_label = test.iloc[:, -1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(train_set)\n",
    "y_train = np.array(train_label)\n",
    "X_test = np.array(test_set)\n",
    "y_test = np.array(test_label)\n",
    "\n",
    "lb = LabelEncoder()\n",
    "y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
    "y_test = np_utils.to_categorical(lb.fit_transform(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2948, 216)\n",
      "(2948, 6)\n",
      "(748, 216)\n",
      "(748, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the data using random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.20      0.32       136\n",
      "           1       0.86      0.54      0.66       104\n",
      "           2       0.89      0.45      0.60       150\n",
      "           3       0.94      0.47      0.63       143\n",
      "           4       0.96      0.52      0.67        93\n",
      "           5       0.96      0.40      0.57       122\n",
      "\n",
      "   micro avg       0.92      0.42      0.58       748\n",
      "   macro avg       0.92      0.43      0.57       748\n",
      "weighted avg       0.91      0.42      0.57       748\n",
      " samples avg       0.42      0.42      0.42       748\n",
      "\n",
      "0.4197860962566845\n",
      "*********************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.21      0.33       136\n",
      "           1       0.86      0.54      0.66       104\n",
      "           2       0.89      0.45      0.60       150\n",
      "           3       0.94      0.47      0.63       143\n",
      "           4       0.96      0.52      0.67        93\n",
      "           5       0.96      0.42      0.58       122\n",
      "\n",
      "   micro avg       0.92      0.43      0.58       748\n",
      "   macro avg       0.92      0.43      0.58       748\n",
      "weighted avg       0.92      0.43      0.57       748\n",
      " samples avg       0.43      0.43      0.43       748\n",
      "\n",
      "0.42513368983957217\n",
      "*********************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.22      0.35       136\n",
      "           1       0.86      0.54      0.66       104\n",
      "           2       0.91      0.45      0.60       150\n",
      "           3       0.94      0.47      0.63       143\n",
      "           4       0.96      0.52      0.67        93\n",
      "           5       0.96      0.42      0.58       122\n",
      "\n",
      "   micro avg       0.92      0.43      0.58       748\n",
      "   macro avg       0.92      0.44      0.58       748\n",
      "weighted avg       0.92      0.43      0.58       748\n",
      " samples avg       0.43      0.43      0.43       748\n",
      "\n",
      "0.42780748663101603\n",
      "*********************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.23      0.36       136\n",
      "           1       0.86      0.54      0.66       104\n",
      "           2       0.91      0.45      0.60       150\n",
      "           3       0.94      0.47      0.63       143\n",
      "           4       0.96      0.52      0.67        93\n",
      "           5       0.96      0.42      0.58       122\n",
      "\n",
      "   micro avg       0.92      0.43      0.59       748\n",
      "   macro avg       0.92      0.44      0.59       748\n",
      "weighted avg       0.92      0.43      0.58       748\n",
      " samples avg       0.43      0.43      0.43       748\n",
      "\n",
      "0.42914438502673796\n",
      "*********************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.23      0.36       136\n",
      "           1       0.86      0.54      0.66       104\n",
      "           2       0.91      0.45      0.60       150\n",
      "           3       0.94      0.47      0.63       143\n",
      "           4       0.96      0.52      0.67        93\n",
      "           5       0.96      0.42      0.58       122\n",
      "\n",
      "   micro avg       0.92      0.43      0.59       748\n",
      "   macro avg       0.92      0.44      0.59       748\n",
      "weighted avg       0.92      0.43      0.58       748\n",
      " samples avg       0.43      0.43      0.43       748\n",
      "\n",
      "0.42914438502673796\n",
      "*********************************************************************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.23      0.36       136\n",
      "           1       0.86      0.54      0.66       104\n",
      "           2       0.91      0.45      0.60       150\n",
      "           3       0.94      0.47      0.63       143\n",
      "           4       0.96      0.52      0.67        93\n",
      "           5       0.96      0.42      0.58       122\n",
      "\n",
      "   micro avg       0.92      0.43      0.59       748\n",
      "   macro avg       0.92      0.44      0.59       748\n",
      "weighted avg       0.92      0.43      0.58       748\n",
      " samples avg       0.43      0.43      0.43       748\n",
      "\n",
      "0.42914438502673796\n",
      "*********************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for i in range(5,11):\n",
    "    random_forest = RandomForestClassifier(criterion=\"gini\", max_depth=i, max_features=0.2, \n",
    "                                 max_leaf_nodes = 100, min_samples_leaf = 60, min_samples_split = 20, \n",
    "                                 n_estimators= 30000, random_state= 5)\n",
    "    random_forest.fit(X_train, y_train)\n",
    "    predictions = random_forest.predict(X_test)\n",
    "    print(classification_report(y_test,predictions))\n",
    "    print(accuracy_score(y_test, predictions))\n",
    "    print('*********************************************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_forest = RandomForestClassifier(criterion=\"gini\", max_depth=10, max_features=0.2, \n",
    "#                                  max_leaf_nodes = 100, min_samples_leaf = 60, min_samples_split = 20, \n",
    "#                                  n_estimators= 30000, random_state= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = random_forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "# print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
